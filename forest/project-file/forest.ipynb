{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "LOGISTIC REGRESSION\n",
      "==============================\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jatin\\Desktop\\forest\\forest_cover_prediction\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'C': 10}\n",
      "Accuracy: 0.6667\n",
      "MSE: 3.0615\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.63      0.65      0.64       432\n",
      "           2       0.57      0.44      0.50       432\n",
      "           3       0.56      0.51      0.54       432\n",
      "           4       0.81      0.91      0.86       432\n",
      "           5       0.62      0.68      0.65       432\n",
      "           6       0.58      0.60      0.59       432\n",
      "           7       0.86      0.87      0.87       432\n",
      "\n",
      "    accuracy                           0.67      3024\n",
      "   macro avg       0.66      0.67      0.66      3024\n",
      "weighted avg       0.66      0.67      0.66      3024\n",
      "\n",
      "Confusion Matrix:\n",
      " [[280  71   1   0  25   1  54]\n",
      " [102 192  19   0  95  19   5]\n",
      " [  0   3 222  60  17 130   0]\n",
      " [  0   0  21 393   0  18   0]\n",
      " [ 11  63  42   0 293  23   0]\n",
      " [  0  10  91  33  38 260   0]\n",
      " [ 53   0   1   0   2   0 376]]\n",
      "\n",
      "==============================\n",
      "DECISION TREE\n",
      "==============================\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best Params: {'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Accuracy: 0.7910\n",
      "MSE: 1.8757\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.68      0.69      0.69       432\n",
      "           2       0.65      0.60      0.62       432\n",
      "           3       0.76      0.74      0.75       432\n",
      "           4       0.91      0.91      0.91       432\n",
      "           5       0.81      0.89      0.85       432\n",
      "           6       0.80      0.81      0.80       432\n",
      "           7       0.91      0.90      0.91       432\n",
      "\n",
      "    accuracy                           0.79      3024\n",
      "   macro avg       0.79      0.79      0.79      3024\n",
      "weighted avg       0.79      0.79      0.79      3024\n",
      "\n",
      "Confusion Matrix:\n",
      " [[297  87   1   0  16   0  31]\n",
      " [ 93 258  12   0  51  12   6]\n",
      " [  0   7 319  30  13  63   0]\n",
      " [  0   1  27 395   0   9   0]\n",
      " [  2  28  12   0 386   4   0]\n",
      " [  2  11  51   7  12 349   0]\n",
      " [ 40   3   0   0   1   0 388]]\n",
      "\n",
      "==============================\n",
      "RANDOM FOREST\n",
      "==============================\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best Params: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Accuracy: 0.8657\n",
      "MSE: 1.1104\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.78      0.78      0.78       432\n",
      "           2       0.80      0.66      0.72       432\n",
      "           3       0.86      0.82      0.84       432\n",
      "           4       0.94      0.98      0.96       432\n",
      "           5       0.89      0.95      0.92       432\n",
      "           6       0.84      0.90      0.87       432\n",
      "           7       0.94      0.97      0.95       432\n",
      "\n",
      "    accuracy                           0.87      3024\n",
      "   macro avg       0.86      0.87      0.86      3024\n",
      "weighted avg       0.86      0.87      0.86      3024\n",
      "\n",
      "Confusion Matrix:\n",
      " [[335  61   1   0   9   1  25]\n",
      " [ 81 285  11   0  33  18   4]\n",
      " [  0   1 355  22   8  46   0]\n",
      " [  0   0   4 424   0   4   0]\n",
      " [  1   6   8   0 411   6   0]\n",
      " [  0   4  33   5   1 389   0]\n",
      " [ 13   0   0   0   0   0 419]]\n",
      "\n",
      "==============================\n",
      "MODEL ACCURACY COMPARISON\n",
      "==============================\n",
      "Logistic Regression Accuracy: 0.6667\n",
      "Decision Tree Accuracy     : 0.7910\n",
      "Random Forest Accuracy     : 0.8657\n",
      "\n",
      "âœ… Best model (RandomForest with accuracy 0.8657) saved to: best_forest_cover_model_RandomForest_0.8657.pkl\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# -----------------------------\n",
    "# Load dataset\n",
    "# !!! IMPORTANT: Replace \"forest_cover_dataset.csv\" with the actual path to your dataset file.\n",
    "# This dataset is expected to have features as described in \"Forest Cover Type Prediction.pdf\"\n",
    "# and the target column named \"Cover_Type\".\n",
    "# Wilderness_Area (4 binary columns) and Soil_Type (40 binary columns) are assumed\n",
    "# to be already in their binary/dummy variable format.\n",
    "try:\n",
    "    df = pd.read_csv(\"train.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'train.csv' not found. Please replace with your actual dataset file path.\")\n",
    "    exit()\n",
    "\n",
    "# -----------------------------\n",
    "# Basic Preprocessing (Minimal, assuming data is mostly ready as per PDF description)\n",
    "\n",
    "# Example: If there are any non-numeric identifier columns not part of features or target, drop them.\n",
    "# For this problem, based on the PDF, all described columns are either features or the target.\n",
    "# If your CSV has an ID column, for example, you would drop it here:\n",
    "# if 'Id' in df.columns:\n",
    "#     df.drop(columns=['Id'], inplace=True)\n",
    "\n",
    "# Handle missing values - a simple approach is to drop rows with any NaNs.\n",
    "# Consider more sophisticated imputation if appropriate for your dataset.\n",
    "df = df.dropna()\n",
    "\n",
    "# Ensure target variable 'Cover_Type' is integer (as per PDF description [cite: 2])\n",
    "if 'Cover_Type' in df.columns:\n",
    "    df['Cover_Type'] = df['Cover_Type'].astype(int)\n",
    "else:\n",
    "    print(\"Error: Target column 'Cover_Type' not found in the dataset.\")\n",
    "    exit()\n",
    "\n",
    "# -----------------------------\n",
    "# Features and Target\n",
    "# 'Cover_Type' is the target variable [cite: 3]\n",
    "# All other columns are features.\n",
    "if 'Cover_Type' not in df.columns:\n",
    "    print(\"Error: Target column 'Cover_Type' is missing from the dataset.\")\n",
    "    exit()\n",
    "\n",
    "X = df.drop(columns=['Cover_Type'])\n",
    "y = df['Cover_Type']\n",
    "\n",
    "# Check for any NaNs left after potential dropna\n",
    "assert X.isnull().sum().sum() == 0, \"There are still NaN values in features X.\"\n",
    "assert y.isnull().sum() == 0, \"There are still NaN values in target y.\"\n",
    "\n",
    "# -----------------------------\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y if y.nunique() > 1 else None)\n",
    "\n",
    "# -----------------------------\n",
    "# Scale features\n",
    "# Features like Elevation, Aspect, Slope, Distances, Hillshades are numerical [cite: 1]\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# =============================\n",
    "# 1. Logistic Regression\n",
    "# =============================\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"LOGISTIC REGRESSION\")\n",
    "print(\"=\"*30)\n",
    "# For multi-class problems, 'ovr' (One-vs-Rest) or 'multinomial' can be used.\n",
    "# 'lbfgs' solver supports multinomial.\n",
    "logreg = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=1000, random_state=42)\n",
    "param_grid_lr = {'C': [0.01, 0.1, 1, 10, 100]} # Expanded C for potentially more complex data\n",
    "grid_lr = GridSearchCV(logreg, param_grid_lr, cv=5, n_jobs=-1, verbose=1, scoring='accuracy')\n",
    "grid_lr.fit(X_train, y_train)\n",
    "y_pred_lr = grid_lr.predict(X_test)\n",
    "\n",
    "print(\"Best Params:\", grid_lr.best_params_)\n",
    "acc_lr = accuracy_score(y_test, y_pred_lr)\n",
    "print(f\"Accuracy: {acc_lr:.4f}\")\n",
    "# MSE is not a primary metric for classification, but included for structural consistency with the example.\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_pred_lr):.4f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_lr))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_lr))\n",
    "\n",
    "# =============================\n",
    "# 2. Decision Tree\n",
    "# =============================\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"DECISION TREE\")\n",
    "print(\"=\"*30)\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "param_grid_dt = {'max_depth': [5, 10, 15, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]}\n",
    "grid_dt = GridSearchCV(dt, param_grid_dt, cv=5, n_jobs=-1, verbose=1, scoring='accuracy')\n",
    "grid_dt.fit(X_train, y_train)\n",
    "y_pred_dt = grid_dt.predict(X_test)\n",
    "\n",
    "print(\"Best Params:\", grid_dt.best_params_)\n",
    "acc_dt = accuracy_score(y_test, y_pred_dt)\n",
    "print(f\"Accuracy: {acc_dt:.4f}\")\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_pred_dt):.4f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_dt))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_dt))\n",
    "\n",
    "# =============================\n",
    "# 3. Random Forest\n",
    "# =============================\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"RANDOM FOREST\")\n",
    "print(\"=\"*30)\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "# Adjusted parameters for potentially larger/more complex dataset\n",
    "param_grid_rf = {'n_estimators': [100, 200],\n",
    "                 'max_depth': [10, 20, None],\n",
    "                 'min_samples_split': [2, 5],\n",
    "                 'min_samples_leaf': [1, 2]}\n",
    "grid_rf = GridSearchCV(rf, param_grid_rf, cv=5, n_jobs=-1, verbose=1, scoring='accuracy')\n",
    "grid_rf.fit(X_train, y_train)\n",
    "y_pred_rf = grid_rf.predict(X_test)\n",
    "\n",
    "print(\"Best Params:\", grid_rf.best_params_)\n",
    "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Accuracy: {acc_rf:.4f}\")\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_pred_rf):.4f}\") # Included for consistency\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_rf))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n",
    "\n",
    "# =============================\n",
    "# ðŸ“Š Accuracy Comparison\n",
    "# =============================\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"MODEL ACCURACY COMPARISON\")\n",
    "print(\"=\"*30)\n",
    "print(f\"Logistic Regression Accuracy: {acc_lr:.4f}\")\n",
    "print(f\"Decision Tree Accuracy     : {acc_dt:.4f}\")\n",
    "print(f\"Random Forest Accuracy     : {acc_rf:.4f}\")\n",
    "\n",
    "# =============================\n",
    "# ðŸ’¾ Save the best model\n",
    "# =============================\n",
    "# Determine the best model based on accuracy\n",
    "models_accuracies = {\n",
    "    \"LogisticRegression\": (grid_lr.best_estimator_, acc_lr),\n",
    "    \"DecisionTree\": (grid_dt.best_estimator_, acc_dt),\n",
    "    \"RandomForest\": (grid_rf.best_estimator_, acc_rf)\n",
    "}\n",
    "\n",
    "best_model_name = max(models_accuracies, key=lambda k: models_accuracies[k][1])\n",
    "best_model_estimator = models_accuracies[best_model_name][0]\n",
    "best_model_accuracy = models_accuracies[best_model_name][1]\n",
    "\n",
    "model_filename = f\"best_forest_cover_model_{best_model_name}_{best_model_accuracy:.4f}.pkl\"\n",
    "joblib.dump(best_model_estimator, model_filename)\n",
    "print(f\"\\nâœ… Best model ({best_model_name} with accuracy {best_model_accuracy:.4f}) saved to: {model_filename}\")\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
